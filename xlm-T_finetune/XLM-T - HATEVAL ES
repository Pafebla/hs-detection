{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XLM-T - HATEVAL ES","provenance":[{"file_id":"1IAA1h8u53O1hi9807u7oOFuT3728N0-n","timestamp":1657626666982},{"file_id":"12JuNVT-j_vQzIF9qEpRXFNqzKXCYgTrB","timestamp":1619639735281},{"file_id":"https://github.com/huggingface/notebooks/blob/master/transformers_doc/pytorch/custom_datasets.ipynb","timestamp":1619569772905}],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nKftOu9fyC8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657674743460,"user_tz":-120,"elapsed":16390,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"2412469b-77d9-4cfb-a95b-7bdb1b72edd3"},"source":["!pip install --upgrade pip\n","!pip install sentencepiece\n","!pip install datasets\n","!pip install transformers"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.1.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"_ykXokStcwGz"},"source":["# Fine-tuning XLM-T\n","\n","This notebook describes a simple case of finetuning. You can finetune either the `XLM-T` language model, or XLM-T sentiment, which has already been fine-tuned on sentiment analysis data, in 8 languages (this could be useful to do sentiment transfer learning on new languages).,\n","\n","This notebook was modified from https://huggingface.co/transformers/custom_datasets.html"]},{"cell_type":"code","metadata":{"id":"Y5f1fFbETSbM","executionInfo":{"status":"ok","timestamp":1657674743461,"user_tz":-120,"elapsed":9,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report"],"execution_count":60,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bQginKBQDwd","executionInfo":{"status":"ok","timestamp":1657674745129,"user_tz":-120,"elapsed":1676,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"95390fc9-4ae0-4453-e3ad-006a3e41c638"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"dtj1poj8yC8b"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"r3OxewRLFaK1","executionInfo":{"status":"ok","timestamp":1657676219790,"user_tz":-120,"elapsed":285,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["EPOCHS = 3\n","LR = 5e-5\n","BATCH_SIZE = 32\n","MODEL = \"cardiffnlp/twitter-xlm-roberta-base\" # use this to finetune the language model\n","#MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the sentiment classifier\n","MAX_TRAINING_EXAMPLES = -1 # set this to -1 if you want to use the whole training set"],"execution_count":117,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWqZ7LGMFeHV"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"a1A24j8pFhup"},"source":["We download the xml-t sentiment dataset (`UMSAB`) but you can use your own.\n","If you use the same files structures as [TweetEval](https://github.com/cardiffnlp/tweeteval) (`train_text.txt`, `train_labels.txt`, `val_text.txt`, `...`), you do not need to change anything in the code.\n","\n","---\n","\n"]},{"cell_type":"code","source":["\n","train_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/es/fixed/preprocessed/sin_emojis/train.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/es/fixed/preprocessed/sin_emojis/test.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","dev_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/es/fixed/preprocessed/sin_emojis/val.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","\n","\n","#y_true = test_df.loc[:,\"label\"]\n","#train_df.to_csv('train.csv')\n","#dev_df.to_csv('val.csv')\n","#test_df.to_csv('test.csv')\n","\n","\n","test_df['label'].to_csv('test_labels.txt', sep='\\n', index=False, header=False)\n","test_df['Tweet'].to_csv('test_text.txt', sep='\\n', index=False, header=False)\n","train_df['label'].to_csv('train_labels.txt', sep='\\n', index=False, header=False)\n","train_df['Tweet'].to_csv('train_text.txt', sep='\\n', index=False, header=False)\n","dev_df['label'].to_csv('val_labels.txt', sep='\\n', index=False, header=False)\n","dev_df['Tweet'].to_csv('val_text.txt', sep='\\n', index=False, header=False)\n","\n","def remove_last_line_from_csv(filename):\n","    with open(filename) as myFile:\n","        lines = myFile.readlines()\n","        last_line = lines[len(lines)-1]\n","        lines[len(lines)-1] = last_line.rstrip()\n","    with open(filename, 'w') as myFile:    \n","        myFile.writelines(lines)\n","\n","remove_last_line_from_csv('test_labels.txt')\n","remove_last_line_from_csv('test_text.txt')\n","remove_last_line_from_csv('train_labels.txt')\n","remove_last_line_from_csv('train_text.txt')\n","remove_last_line_from_csv('val_labels.txt')\n","remove_last_line_from_csv('val_text.txt')"],"metadata":{"id":"wKaIZ7YWPd7C","executionInfo":{"status":"ok","timestamp":1657676220159,"user_tz":-120,"elapsed":380,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvM2mCi2yC8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657676221748,"user_tz":-120,"elapsed":1592,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"6bff8f3e-f664-40f1-9292-b8c594a6ea8f"},"source":["# loading dataset for UMSAB's all 8 languages\n","\n","files = \"\"\"test_labels.txt\n","test_text.txt\n","train_labels.txt\n","train_text.txt\n","val_labels.txt\n","val_text.txt\"\"\".split('\\n')\n","\n","for f in files:\n","  p = f\"/content/{f}\"\n","  !wget $p"],"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/test_labels.txt: Scheme missing.\n","/content/test_text.txt: Scheme missing.\n","/content/train_labels.txt: Scheme missing.\n","/content/train_text.txt: Scheme missing.\n","/content/val_labels.txt: Scheme missing.\n","/content/val_text.txt: Scheme missing.\n"]}]},{"cell_type":"code","metadata":{"id":"DqNPXQg7GSn3","executionInfo":{"status":"ok","timestamp":1657676221749,"user_tz":-120,"elapsed":3,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["dataset_dict = {}\n","for i in ['train','val','test']:\n","  dataset_dict[i] = {}\n","  for j in ['text','labels']:\n","    dataset_dict[i][j] = open(f\"{i}_{j}.txt\").read().split('\\n')\n","    if j == 'labels':\n","      dataset_dict[i][j] = [int(x) for x in dataset_dict[i][j]]\n","\n","if MAX_TRAINING_EXAMPLES > 0:\n","  dataset_dict['train']['text']=dataset_dict['train']['text'][:MAX_TRAINING_EXAMPLES]\n","  dataset_dict['train']['labels']=dataset_dict['train']['labels'][:MAX_TRAINING_EXAMPLES]"],"execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IjMOsNSyC8d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657676225001,"user_tz":-120,"elapsed":3255,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"e41aa36e-a4a2-4cf9-8135-e97f34b64469"},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"],"execution_count":121,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/6e74f475a3ac17548fcd7f24a1116652919ba2225b290de8ef3e7d4ad36393fb.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/c949b67b285c79e2b38b50fb89ca8d00428266f364d9fe0c10721a0492dbbeba.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Rp0llWQVyC8e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657676225452,"user_tz":-120,"elapsed":478,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"0caa6a50-693a-41c4-b7ee-66ea0d9bf300"},"source":["train_encodings = tokenizer(dataset_dict['train']['text'], truncation=True, padding=True)\n","val_encodings = tokenizer(dataset_dict['val']['text'], truncation=True, padding=True)\n","test_encodings = tokenizer(dataset_dict['test']['text'], truncation=True, padding=True)"],"execution_count":122,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}]},{"cell_type":"code","metadata":{"id":"1MzkHFG5yC8f","executionInfo":{"status":"ok","timestamp":1657676225830,"user_tz":-120,"elapsed":11,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = MyDataset(train_encodings, dataset_dict['train']['labels'])\n","val_dataset = MyDataset(val_encodings, dataset_dict['val']['labels'])\n","test_dataset = MyDataset(test_encodings, dataset_dict['test']['labels'])"],"execution_count":123,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_BTQBaJyC8g"},"source":["## Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"zmp35MgkyC8g"},"source":["The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model\n","to fine-tune, define the `TrainingArguments`/`TFTrainingArguments` and\n","instantiate a `Trainer`/`TFTrainer`."]},{"cell_type":"code","metadata":{"id":"PGuho0dMyC8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657676228936,"user_tz":-120,"elapsed":3116,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"6bc26636-45e6-4583-8736-c9fa03a33670"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',                   # output directory\n","    num_train_epochs=EPOCHS,                  # total number of training epochs\n","    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n","    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,                        # strength of weight decay\n","    logging_dir='./logs',                     # directory for storing logs\n","    logging_steps=10,                         # when to print log\n","    #load_best_model_at_end=True,              # load or not best model at the end\n",")\n","\n","num_labels = len(set(dataset_dict[\"train\"][\"labels\"]))\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)"],"execution_count":124,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de8103490e7e03ee5861271a32167a4f170ce820c6ab3aba4df4d6b00fe65163.824322f81a7eced2993b9bf64487652aa08c7cd2a81b38e00b0464c51b97280d\n","Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"J7bArzixEAH-","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1657676380614,"user_tz":-120,"elapsed":151699,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"a945914f-21f7-4239-f470-573597dcc771"},"source":["trainer = Trainer(\n","    model=model,                              # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                       # training arguments, defined above\n","    train_dataset=train_dataset,              # training dataset\n","    eval_dataset=val_dataset                  # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":125,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4466\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 420\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [420/420 02:30, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.710900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.696500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.687000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.665300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.694500</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.649500</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.534300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.542800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.506500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.497400</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.443400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.383700</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.473500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.388100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.341100</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.399300</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.359000</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.355900</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.297000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.329100</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.358500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.363400</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.345800</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.382800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.284600</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.339900</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.338300</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.319500</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.226600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.199000</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.165200</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.186300</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.127500</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.158500</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.130800</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.155900</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.223900</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.184400</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.241300</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.144300</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.163100</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.216000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=420, training_loss=0.3621476689974467, metrics={'train_runtime': 151.3306, 'train_samples_per_second': 88.535, 'train_steps_per_second': 2.775, 'total_flos': 729818678690640.0, 'train_loss': 0.3621476689974467, 'epoch': 3.0})"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"Db-zlWQLXEVf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657676385318,"user_tz":-120,"elapsed":4736,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"bf5cd264-da6a-4147-d6e1-a8db779397f1"},"source":["trainer.save_model(\"./results/best_model\") # save best model"],"execution_count":126,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/best_model\n","Configuration saved in ./results/best_model/config.json\n","Model weights saved in ./results/best_model/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"Kr3--ZKNbn1t"},"source":["## Evaluate on Test set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"hqeWwkSCXaKV","executionInfo":{"status":"ok","timestamp":1657676391736,"user_tz":-120,"elapsed":6446,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"6d3ac04a-e489-479b-b42f-da2f4d4b17cf"},"source":["\n","\n","from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n","\n","trainer.evaluate()\n","\n","raw_pred, test_labels, _ = trainer.predict(test_dataset) \n","y_pred = np.argmax(raw_pred, axis=1)\n","\n","lista_metricas = []\n","lista_metricas.append(f1_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(accuracy_score(test_labels, y_pred))\n","lista_metricas.append(precision_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(recall_score(test_labels, y_pred, average='macro'))\n","\n","lista_metricas. append(\"epochs: \"+ str(EPOCHS))\n","lista_metricas. append(\"learning_rate: \"+ str(LR))\n","lista_metricas. append(\"batch size: \"+ str(BATCH_SIZE))\n","#lista_metricas. append(\"features: \"+ str(features))\n","\n","\n","print(lista_metricas)\n","df_metricas = pd.DataFrame(lista_metricas)\n","df_metricas.to_excel('metricas.xlsx', sheet_name='sheet1', index=False)"],"execution_count":127,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 500\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='66' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16/16 00:06]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1600\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["[0.7539246771933095, 0.755625, 0.7564102564102564, 0.7644906511927789, 'epochs: 3', 'learning_rate: 5e-05', 'batch size: 32']\n"]}]},{"cell_type":"markdown","metadata":{"id":"rLR23BxIyC8g"},"source":["<a id='ft_native'></a>"]}]}