{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XLM-T - HATEVAL EN","provenance":[{"file_id":"18lkU81eLj9GmpKQMz0PP7L1znDnq-Kbz","timestamp":1658255995408},{"file_id":"1IAA1h8u53O1hi9807u7oOFuT3728N0-n","timestamp":1657626666982},{"file_id":"12JuNVT-j_vQzIF9qEpRXFNqzKXCYgTrB","timestamp":1619639735281},{"file_id":"https://github.com/huggingface/notebooks/blob/master/transformers_doc/pytorch/custom_datasets.ipynb","timestamp":1619569772905}],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nKftOu9fyC8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658256123928,"user_tz":-120,"elapsed":44764,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"593815bc-7f95-4021-e217-ece186630678"},"source":["!pip install --upgrade pip\n","!pip install sentencepiece\n","!pip install datasets\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 7.4 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.1.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.8/271.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, urllib3, pyyaml, multidict, fsspec, frozenlist, asynctest, async-timeout, yarl, aiosignal, responses, huggingface-hub, aiohttp, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.12.1 transformers-4.20.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"_ykXokStcwGz"},"source":["# Fine-tuning XLM-T\n","\n","This notebook describes a simple case of finetuning. You can finetune either the `XLM-T` language model, or XLM-T sentiment, which has already been fine-tuned on sentiment analysis data, in 8 languages (this could be useful to do sentiment transfer learning on new languages).,\n","\n","This notebook was modified from https://huggingface.co/transformers/custom_datasets.html"]},{"cell_type":"code","metadata":{"id":"Y5f1fFbETSbM","executionInfo":{"status":"ok","timestamp":1658256133168,"user_tz":-120,"elapsed":9248,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bQginKBQDwd","executionInfo":{"status":"ok","timestamp":1658256151058,"user_tz":-120,"elapsed":17909,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"d995874a-d389-4424-c224-1574c8ce48f8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"dtj1poj8yC8b"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"r3OxewRLFaK1","executionInfo":{"status":"ok","timestamp":1658276696914,"user_tz":-120,"elapsed":248,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["EPOCHS = 3\n","LR = 5e-5\n","BATCH_SIZE = 64\n","MODEL = \"cardiffnlp/twitter-xlm-roberta-base\" # use this to finetune the language model\n","#MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the sentiment classifier\n","MAX_TRAINING_EXAMPLES = -1 # set this to -1 if you want to use the whole training set"],"execution_count":210,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWqZ7LGMFeHV"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"a1A24j8pFhup"},"source":["We download the xml-t sentiment dataset (`UMSAB`) but you can use your own.\n","If you use the same files structures as [TweetEval](https://github.com/cardiffnlp/tweeteval) (`train_text.txt`, `train_labels.txt`, `val_text.txt`, `...`), you do not need to change anything in the code.\n","\n","---\n","\n"]},{"cell_type":"code","source":["\n","train_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/en/fixed/train.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","test_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/en/fixed/test.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","dev_df = pd.read_csv(\"/content/drive/MyDrive/data/hateval/en/fixed/dev.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","\n","#y_true = test_df.loc[:,\"label\"]\n","#train_df.to_csv('train.csv')\n","#dev_df.to_csv('val.csv')\n","#test_df.to_csv('test.csv')\n","\n","\n","test_df['label'].to_csv('test_labels.txt', sep='\\n', index=False, header=False)\n","test_df['Tweet'].to_csv('test_text.txt', sep='\\n', index=False, header=False)\n","train_df['label'].to_csv('train_labels.txt', sep='\\n', index=False, header=False)\n","train_df['Tweet'].to_csv('train_text.txt', sep='\\n', index=False, header=False)\n","dev_df['label'].to_csv('val_labels.txt', sep='\\n', index=False, header=False)\n","dev_df['Tweet'].to_csv('val_text.txt', sep='\\n', index=False, header=False)\n","\n","def remove_last_line_from_csv(filename):\n","    with open(filename) as myFile:\n","        lines = myFile.readlines()\n","        last_line = lines[len(lines)-1]\n","        lines[len(lines)-1] = last_line.rstrip()\n","    with open(filename, 'w') as myFile:    \n","        myFile.writelines(lines)\n","\n","remove_last_line_from_csv('test_labels.txt')\n","remove_last_line_from_csv('test_text.txt')\n","remove_last_line_from_csv('train_labels.txt')\n","remove_last_line_from_csv('train_text.txt')\n","remove_last_line_from_csv('val_labels.txt')\n","remove_last_line_from_csv('val_text.txt')"],"metadata":{"id":"wKaIZ7YWPd7C","executionInfo":{"status":"ok","timestamp":1658276697719,"user_tz":-120,"elapsed":402,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"execution_count":211,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvM2mCi2yC8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658276699672,"user_tz":-120,"elapsed":1956,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"e259e13e-c282-41ec-c247-3a590637795c"},"source":["# loading dataset for UMSAB's all 8 languages\n","\n","files = \"\"\"test_labels.txt\n","test_text.txt\n","train_labels.txt\n","train_text.txt\n","val_labels.txt\n","val_text.txt\"\"\".split('\\n')\n","\n","for f in files:\n","  p = f\"/content/{f}\"\n","  !wget $p"],"execution_count":212,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/test_labels.txt: Scheme missing.\n","/content/test_text.txt: Scheme missing.\n","/content/train_labels.txt: Scheme missing.\n","/content/train_text.txt: Scheme missing.\n","/content/val_labels.txt: Scheme missing.\n","/content/val_text.txt: Scheme missing.\n"]}]},{"cell_type":"code","metadata":{"id":"DqNPXQg7GSn3","executionInfo":{"status":"ok","timestamp":1658276699980,"user_tz":-120,"elapsed":312,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["dataset_dict = {}\n","for i in ['train','val','test']:\n","  dataset_dict[i] = {}\n","  for j in ['text','labels']:\n","    dataset_dict[i][j] = open(f\"{i}_{j}.txt\").read().split('\\n')\n","    if j == 'labels':\n","      dataset_dict[i][j] = [int(x) for x in dataset_dict[i][j]]\n","\n","if MAX_TRAINING_EXAMPLES > 0:\n","  dataset_dict['train']['text']=dataset_dict['train']['text'][:MAX_TRAINING_EXAMPLES]\n","  dataset_dict['train']['labels']=dataset_dict['train']['labels'][:MAX_TRAINING_EXAMPLES]"],"execution_count":213,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IjMOsNSyC8d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658276703739,"user_tz":-120,"elapsed":3773,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"43932384-276b-416e-9776-4fbd3067fe93"},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"],"execution_count":214,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/6e74f475a3ac17548fcd7f24a1116652919ba2225b290de8ef3e7d4ad36393fb.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/c949b67b285c79e2b38b50fb89ca8d00428266f364d9fe0c10721a0492dbbeba.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Rp0llWQVyC8e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658276705487,"user_tz":-120,"elapsed":1760,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"e978d4d8-7e59-4c13-8a03-3e69ac3eb883"},"source":["train_encodings = tokenizer(dataset_dict['train']['text'], truncation=True, padding=True)\n","val_encodings = tokenizer(dataset_dict['val']['text'], truncation=True, padding=True)\n","test_encodings = tokenizer(dataset_dict['test']['text'], truncation=True, padding=True)"],"execution_count":215,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}]},{"cell_type":"code","metadata":{"id":"1MzkHFG5yC8f","executionInfo":{"status":"ok","timestamp":1658276705489,"user_tz":-120,"elapsed":20,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = MyDataset(train_encodings, dataset_dict['train']['labels'])\n","val_dataset = MyDataset(val_encodings, dataset_dict['val']['labels'])\n","test_dataset = MyDataset(test_encodings, dataset_dict['test']['labels'])"],"execution_count":216,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_BTQBaJyC8g"},"source":["## Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"zmp35MgkyC8g"},"source":["The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model\n","to fine-tune, define the `TrainingArguments`/`TFTrainingArguments` and\n","instantiate a `Trainer`/`TFTrainer`."]},{"cell_type":"code","metadata":{"id":"PGuho0dMyC8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658276709679,"user_tz":-120,"elapsed":4209,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"089e5555-a083-46b6-fb97-43bc52b50a5a"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',                   # output directory\n","    num_train_epochs=EPOCHS,                  # total number of training epochs\n","    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n","    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,                        # strength of weight decay\n","    logging_dir='./logs',                     # directory for storing logs\n","    logging_steps=10,                         # when to print log\n","    #load_best_model_at_end=True,              # load or not best model at the end\n",")\n","\n","num_labels = len(set(dataset_dict[\"train\"][\"labels\"]))\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)"],"execution_count":217,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de8103490e7e03ee5861271a32167a4f170ce820c6ab3aba4df4d6b00fe65163.824322f81a7eced2993b9bf64487652aa08c7cd2a81b38e00b0464c51b97280d\n","Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"J7bArzixEAH-","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1658276945417,"user_tz":-120,"elapsed":235760,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"8ac47871-9530-44eb-c761-d9a8e52e441e"},"source":["trainer = Trainer(\n","    model=model,                              # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                       # training arguments, defined above\n","    train_dataset=train_dataset,              # training dataset\n","    eval_dataset=val_dataset                  # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":218,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 8000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 375\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [375/375 03:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.716300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.697400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.684100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.679500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.631600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.543900</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.515200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.518400</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.472600</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.494900</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.465800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.485400</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.405200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.413800</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.401600</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.420700</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.419100</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.376100</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.385300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.382900</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.343100</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.385500</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.356100</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.384400</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.391900</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.246800</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.264200</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.257800</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.257300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.243900</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.230400</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.229900</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.220600</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.222000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.227800</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.249500</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.214900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=375, training_loss=0.3990069014231364, metrics={'train_runtime': 235.0957, 'train_samples_per_second': 102.086, 'train_steps_per_second': 1.595, 'total_flos': 1331999717760000.0, 'train_loss': 0.3990069014231364, 'epoch': 3.0})"]},"metadata":{},"execution_count":218}]},{"cell_type":"code","metadata":{"id":"Db-zlWQLXEVf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658276950277,"user_tz":-120,"elapsed":4892,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"3e792463-bd41-4b53-d879-50dfcf7f2b48"},"source":["trainer.save_model(\"./results/best_model\") # save best model"],"execution_count":219,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/best_model\n","Configuration saved in ./results/best_model/config.json\n","Model weights saved in ./results/best_model/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"Kr3--ZKNbn1t"},"source":["## Evaluate on Test set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"hqeWwkSCXaKV","executionInfo":{"status":"ok","timestamp":1658276960166,"user_tz":-120,"elapsed":9907,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"348aa57e-83ee-4bdb-ac15-7d8019114c16"},"source":["\n","\n","from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n","\n","trainer.evaluate()\n","\n","raw_pred, test_labels, _ = trainer.predict(test_dataset) \n","y_pred = np.argmax(raw_pred, axis=1)\n","\n","lista_metricas = []\n","lista_metricas.append(f1_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(accuracy_score(test_labels, y_pred))\n","lista_metricas.append(precision_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(recall_score(test_labels, y_pred, average='macro'))\n","\n","lista_metricas. append(\"epochs: \"+ str(EPOCHS))\n","lista_metricas. append(\"learning_rate: \"+ str(LR))\n","lista_metricas. append(\"batch size: \"+ str(BATCH_SIZE))\n","#lista_metricas. append(\"features: \"+ str(features))\n","\n","\n","print(lista_metricas)\n","df_metricas = pd.DataFrame(lista_metricas)\n","df_metricas.to_excel('metricas.xlsx', sheet_name='sheet1', index=False)"],"execution_count":220,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='53' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [16/16 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 2333\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["[0.481320077728585, 0.4813544792113159, 0.6038030370129086, 0.6057405698627073, 'epochs: 3', 'learning_rate: 5e-05', 'batch size: 64']\n"]}]},{"cell_type":"markdown","metadata":{"id":"rLR23BxIyC8g"},"source":["<a id='ft_native'></a>"]}]}