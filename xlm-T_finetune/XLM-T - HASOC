{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" XLM-T - HASOC","provenance":[{"file_id":"1U66VYDSVBjl5-Y9OcSOCG6TAcJOfyMGt","timestamp":1658271568008},{"file_id":"18lkU81eLj9GmpKQMz0PP7L1znDnq-Kbz","timestamp":1658255995408},{"file_id":"1IAA1h8u53O1hi9807u7oOFuT3728N0-n","timestamp":1657626666982},{"file_id":"12JuNVT-j_vQzIF9qEpRXFNqzKXCYgTrB","timestamp":1619639735281},{"file_id":"https://github.com/huggingface/notebooks/blob/master/transformers_doc/pytorch/custom_datasets.ipynb","timestamp":1619569772905}],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nKftOu9fyC8R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658781802677,"user_tz":-120,"elapsed":32803,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"55f013f8-7c43-4b57-e759-ce29945bc0b5"},"source":["!pip install --upgrade pip\n","!pip install sentencepiece\n","!pip install datasets\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 11.9 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fsspec[http]>=2021.11.1\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.8/271.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, urllib3, pyyaml, multidict, fsspec, frozenlist, asynctest, async-timeout, yarl, aiosignal, responses, huggingface-hub, aiohttp, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.4.0 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.12.1 transformers-4.20.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"_ykXokStcwGz"},"source":["# Fine-tuning XLM-T\n","\n","This notebook describes a simple case of finetuning. You can finetune either the `XLM-T` language model, or XLM-T sentiment, which has already been fine-tuned on sentiment analysis data, in 8 languages (this could be useful to do sentiment transfer learning on new languages).,\n","\n","This notebook was modified from https://huggingface.co/transformers/custom_datasets.html"]},{"cell_type":"code","metadata":{"id":"Y5f1fFbETSbM","executionInfo":{"status":"ok","timestamp":1658781809345,"user_tz":-120,"elapsed":6678,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bQginKBQDwd","executionInfo":{"status":"ok","timestamp":1658781829859,"user_tz":-120,"elapsed":20525,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"6a2ecede-1593-4257-85ed-c657f44e9682"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"dtj1poj8yC8b"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"r3OxewRLFaK1","executionInfo":{"status":"ok","timestamp":1658783165864,"user_tz":-120,"elapsed":323,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["EPOCHS = 10\n","LR = 2e-5\n","BATCH_SIZE = 32\n","MODEL = \"cardiffnlp/twitter-xlm-roberta-base\" # use this to finetune the language model\n","#MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the sentiment classifier\n","MAX_TRAINING_EXAMPLES = -1 # set this to -1 if you want to use the whole training set"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWqZ7LGMFeHV"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"a1A24j8pFhup"},"source":["We download the xml-t sentiment dataset (`UMSAB`) but you can use your own.\n","If you use the same files structures as [TweetEval](https://github.com/cardiffnlp/tweeteval) (`train_text.txt`, `train_labels.txt`, `val_text.txt`, `...`), you do not need to change anything in the code.\n","\n","---\n","\n"]},{"cell_type":"code","source":["train_df = pd.read_csv(\"/content/drive/MyDrive/data/hasoc/preprocessed/train.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","\n","train_df['task_1'].replace({'HOF': 1}, inplace=True)\n","train_df['task_1'].replace({'NOT': 0}, inplace=True)\n","train_df.head(5)\n","train_df.drop('task_2', inplace=True, axis=1)\n","train_df.drop('task_3', inplace=True, axis=1)\n","train_df['categorical'] = 'cat'\n","train_df['polarity'] = 0\n","train_df['subjectivity'] = 0\n","train_df['Target'] = 'Hate'\n","test_df = pd.read_csv(\"/content/drive/MyDrive/data/hasoc/preprocessed/test.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","test_df['task_1'].replace({'HOF': 1}, inplace=True)\n","test_df['task_1'].replace({'NOT': 0}, inplace=True)\n","test_df.drop('task_2', inplace=True, axis=1)\n","test_df.drop('task_3\\r', inplace=True, axis=1)\n","test_df['categorical'] = 'cat'\n","test_df['polarity'] = 0\n","test_df['subjectivity'] = 0\n","test_df['Target'] = 'Hate'\n","dev_df = pd.read_csv(\"/content/drive/MyDrive/data/hasoc/preprocessed/dev.csv\",sep= ',', lineterminator='\\n', encoding=\"utf-8\")\n","dev_df['task_1'].replace({'HOF': 1}, inplace=True)\n","dev_df['task_1'].replace({'NOT': 0}, inplace=True)\n","dev_df.drop('task_2', inplace=True, axis=1)\n","dev_df.drop('task_3', inplace=True, axis=1)\n","dev_df['categorical'] = 'cat'\n","dev_df['polarity'] = 0\n","dev_df['subjectivity'] = 0\n","dev_df['Target'] = 'Hate'\n","\n","#y_true = test_df.loc[:,\"label\"]\n","#train_df.to_csv('train.csv')\n","#dev_df.to_csv('val.csv')\n","#test_df.to_csv('test.csv')\n","\n","\n","test_df['task_1'].to_csv('test_labels.txt', sep='\\n', index=False, header=False)\n","test_df['text'].to_csv('test_text.txt', sep='\\n', index=False, header=False)\n","train_df['task_1'].to_csv('train_labels.txt', sep='\\n', index=False, header=False)\n","train_df['text'].to_csv('train_text.txt', sep='\\n', index=False, header=False)\n","dev_df['task_1'].to_csv('val_labels.txt', sep='\\n', index=False, header=False)\n","dev_df['text'].to_csv('val_text.txt', sep='\\n', index=False, header=False)\n","\n","def remove_last_line_from_csv(filename):\n","    with open(filename) as myFile:\n","        lines = myFile.readlines()\n","        last_line = lines[len(lines)-1]\n","        lines[len(lines)-1] = last_line.rstrip()\n","    with open(filename, 'w') as myFile:    \n","        myFile.writelines(lines)\n","\n","remove_last_line_from_csv('test_labels.txt')\n","remove_last_line_from_csv('test_text.txt')\n","remove_last_line_from_csv('train_labels.txt')\n","remove_last_line_from_csv('train_text.txt')\n","remove_last_line_from_csv('val_labels.txt')\n","remove_last_line_from_csv('val_text.txt')"],"metadata":{"id":"wKaIZ7YWPd7C","executionInfo":{"status":"ok","timestamp":1658783166687,"user_tz":-120,"elapsed":497,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvM2mCi2yC8c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658783167759,"user_tz":-120,"elapsed":1088,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"48c7eaab-63c5-453c-eb21-62bd27ea6fd1"},"source":["# loading dataset for UMSAB's all 8 languages\n","\n","files = \"\"\"test_labels.txt\n","test_text.txt\n","train_labels.txt\n","train_text.txt\n","val_labels.txt\n","val_text.txt\"\"\".split('\\n')\n","\n","for f in files:\n","  p = f\"/content/{f}\"\n","  !wget $p"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/test_labels.txt: Scheme missing.\n","/content/test_text.txt: Scheme missing.\n","/content/train_labels.txt: Scheme missing.\n","/content/train_text.txt: Scheme missing.\n","/content/val_labels.txt: Scheme missing.\n","/content/val_text.txt: Scheme missing.\n"]}]},{"cell_type":"code","metadata":{"id":"DqNPXQg7GSn3","executionInfo":{"status":"ok","timestamp":1658783167760,"user_tz":-120,"elapsed":5,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["dataset_dict = {}\n","for i in ['train','val','test']:\n","  dataset_dict[i] = {}\n","  for j in ['text','labels']:\n","    dataset_dict[i][j] = open(f\"{i}_{j}.txt\").read().split('\\n')\n","    if j == 'labels':\n","      dataset_dict[i][j] = [int(x) for x in dataset_dict[i][j]]\n","\n","if MAX_TRAINING_EXAMPLES > 0:\n","  dataset_dict['train']['text']=dataset_dict['train']['text'][:MAX_TRAINING_EXAMPLES]\n","  dataset_dict['train']['labels']=dataset_dict['train']['labels'][:MAX_TRAINING_EXAMPLES]"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IjMOsNSyC8d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658783171622,"user_tz":-120,"elapsed":3867,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"b2f6bcbf-a07c-41a9-9cbe-995250b91a58"},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/6e74f475a3ac17548fcd7f24a1116652919ba2225b290de8ef3e7d4ad36393fb.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/c949b67b285c79e2b38b50fb89ca8d00428266f364d9fe0c10721a0492dbbeba.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Rp0llWQVyC8e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658783172570,"user_tz":-120,"elapsed":976,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"1d5ca0ec-9b03-4eba-b4d4-f5a3e587f9ce"},"source":["train_encodings = tokenizer(dataset_dict['train']['text'], truncation=True, padding=True)\n","val_encodings = tokenizer(dataset_dict['val']['text'], truncation=True, padding=True)\n","test_encodings = tokenizer(dataset_dict['test']['text'], truncation=True, padding=True)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}]},{"cell_type":"code","metadata":{"id":"1MzkHFG5yC8f","executionInfo":{"status":"ok","timestamp":1658783172571,"user_tz":-120,"elapsed":8,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = MyDataset(train_encodings, dataset_dict['train']['labels'])\n","val_dataset = MyDataset(val_encodings, dataset_dict['val']['labels'])\n","test_dataset = MyDataset(test_encodings, dataset_dict['test']['labels'])"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_BTQBaJyC8g"},"source":["## Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"zmp35MgkyC8g"},"source":["The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model\n","to fine-tune, define the `TrainingArguments`/`TFTrainingArguments` and\n","instantiate a `Trainer`/`TFTrainer`."]},{"cell_type":"code","metadata":{"id":"PGuho0dMyC8g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658783177490,"user_tz":-120,"elapsed":4926,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"4bfc80a6-1435-4c6e-b21b-9d800d7a3a75"},"source":["training_args = TrainingArguments(\n","    output_dir='./results',                   # output directory\n","    num_train_epochs=EPOCHS,                  # total number of training epochs\n","    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n","    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,                        # strength of weight decay\n","    logging_dir='./logs',                     # directory for storing logs\n","    logging_steps=10,                         # when to print log\n","    #load_best_model_at_end=True,              # load or not best model at the end\n",")\n","\n","num_labels = len(set(dataset_dict[\"train\"][\"labels\"]))\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a6b90a261219601cbdef14759cac8d96efe5cca8d364560ac79d3c2acbabd35c.f4d0ffccbbe43c94133958daf6651c5bf05e8ec5de8efd46ddab90dd67b50fb2\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/de8103490e7e03ee5861271a32167a4f170ce820c6ab3aba4df4d6b00fe65163.824322f81a7eced2993b9bf64487652aa08c7cd2a81b38e00b0464c51b97280d\n","Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"J7bArzixEAH-","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b80c36bf-c355-411c-b9a9-37cba87a0cdc","executionInfo":{"status":"ok","timestamp":1658783869074,"user_tz":-120,"elapsed":691615,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}}},"source":["trainer = Trainer(\n","    model=model,                              # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                       # training arguments, defined above\n","    train_dataset=train_dataset,              # training dataset\n","    eval_dataset=val_dataset                  # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 4681\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1470\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1470' max='1470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1470/1470 11:30, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.695800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.679500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.663700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.670500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.649300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.629200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.631100</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.596300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.629000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.609600</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.627500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.605500</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.637700</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.659000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.648500</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.569500</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.539400</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.612900</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.604900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.574200</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.602700</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.536700</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.563000</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.584800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.488600</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.574100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.574000</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.585900</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.612700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.511000</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.505300</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.466200</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.463800</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.426500</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.378600</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.502600</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.445200</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.433800</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.423100</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.457600</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.500300</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.436100</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.432700</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.472500</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.256600</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.328200</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.360500</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.316500</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.317400</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.234300</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.271200</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.240800</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.286800</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.237100</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.315600</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.241200</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.293800</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.211000</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.306100</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.125600</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.194500</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.139500</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.133800</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.230800</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.098900</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.096400</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.196500</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.142100</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.149300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.199100</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.130900</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.266500</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.169400</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.094700</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.102400</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.069000</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.133900</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.120400</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.128500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.080300</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.138800</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.067200</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.118200</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.166800</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.097300</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.119900</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.155100</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.119300</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.072000</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.091400</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.088400</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.089100</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.018000</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.073300</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.118300</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.121300</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.025500</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.099000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.115400</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.131800</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.051100</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.115000</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.043300</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.053000</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>0.072000</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>0.064400</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.043600</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>0.099700</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.057800</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.078900</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>0.076800</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>0.075600</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.041400</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.063500</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>0.114300</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.050100</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.068600</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.043600</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.024800</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>0.030400</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>0.052300</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.060100</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>0.023900</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.052100</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.092500</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>0.041400</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>0.030100</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.071500</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.081100</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>0.047700</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>0.028000</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>0.063000</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>0.025200</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.042500</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>0.046800</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>0.043500</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>0.064800</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.063000</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>0.039700</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>0.032100</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>0.070100</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>0.027300</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.023800</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>0.045200</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>0.032400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1470, training_loss=0.24696108818586385, metrics={'train_runtime': 691.3405, 'train_samples_per_second': 67.709, 'train_steps_per_second': 2.126, 'total_flos': 3512049533602800.0, 'train_loss': 0.24696108818586385, 'epoch': 10.0})"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"Db-zlWQLXEVf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658783873825,"user_tz":-120,"elapsed":4762,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"1adff989-a952-4b53-9ad4-8aec7f41b72e"},"source":["trainer.save_model(\"./results/best_model\") # save best model"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/best_model\n","Configuration saved in ./results/best_model/config.json\n","Model weights saved in ./results/best_model/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"Kr3--ZKNbn1t"},"source":["## Evaluate on Test set"]},{"cell_type":"code","metadata":{"id":"hqeWwkSCXaKV","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1658783881926,"user_tz":-120,"elapsed":8130,"user":{"displayName":"Pablo Felipe Blanco","userId":"05178669208628057057"}},"outputId":"479a7104-90d7-480d-b6e5-916a6128a9b6"},"source":["\n","\n","from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n","\n","trainer.evaluate()\n","\n","raw_pred, test_labels, _ = trainer.predict(test_dataset) \n","y_pred = np.argmax(raw_pred, axis=1)\n","\n","lista_metricas = []\n","lista_metricas.append(f1_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(accuracy_score(test_labels, y_pred))\n","lista_metricas.append(precision_score(test_labels, y_pred, average='macro'))\n","lista_metricas.append(recall_score(test_labels, y_pred, average='macro'))\n","\n","lista_metricas. append(\"epochs: \"+ str(EPOCHS))\n","lista_metricas. append(\"learning_rate: \"+ str(LR))\n","lista_metricas. append(\"batch size: \"+ str(BATCH_SIZE))\n","#lista_metricas. append(\"features: \"+ str(features))\n","\n","\n","print(lista_metricas)\n","df_metricas = pd.DataFrame(lista_metricas)\n","df_metricas.to_excel('metricas.xlsx', sheet_name='sheet1', index=False)"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1171\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='74' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [37/37 00:08]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Prediction *****\n","  Num examples = 1153\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["[0.7257757631718851, 0.764093668690373, 0.7154057414611948, 0.7698157514450867, 'epochs: 10', 'learning_rate: 2e-05', 'batch size: 32']\n"]}]},{"cell_type":"markdown","metadata":{"id":"rLR23BxIyC8g"},"source":["<a id='ft_native'></a>"]}]}